---
title: "Group 9 Final Project"
author: "Vinny Dee, Zoe Tuan, Yuchen Mao, Tracy Chen, Jake Li"
date: "10/15/2021"
output: 
  html_document: 
    highlight: tango
    theme: united
    toc: true
    toc_depth: 2
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Background


Our business questions are: 1. What factors are most important in determining employee turnover? and 2. What is the best model for predicting turnover? By answering these questions, we can help employers gain an insight into which of the fators affect employee turnover and work on those factors to decrease attrition. Our audience are the employers and the important decision that needs to be made is how to improve employee retention, which they can answer from looking at the eventual results of our model. 


Our dataset comes from  https://www.kaggle.com/davinwijaya/employee-turnover, which is a real dataset shared from Edward Babushkin's blog and contains data concerning employee turnover, the length of time the employees have worked for their companies, as well as gender, age, industry, profession, and scores for employee personality traits such as extraversion, anxiety, self-control, or independence. The dataset contains 16 columns and 1129 rows, and we've taken turnover as the response value, while the other columns are treated as predictors. 


So far, we have not run into any data cleaning issues. The size of the dataset is manageable, and we have not found any missing or NA values.


Employee attrition is an important aspect for all businesses, and finding out which factors attribute most to whether or not an employee is likely to stay with the company is useful for employers everywhere. By having this model and being able to predict which employee's have a high chance of turnover, employers can divert resources to prevent that employee from leaving. It's advantageous to the company, both in conserving time and cost, in recruiting and training new employees if it's possible to maintain the current employee base.

# Analysis

```{r}
turnover <- read.csv("turnover.csv", stringsAsFactors = TRUE, fileEncoding = "latin1")

summary(turnover)

str(turnover)

##As seen in the structure, all data types are correct and thus no additional work is required.

summary(turnover)

##As seen in the summary, our data does not report any missing values, and hence we are done performing an initial clean of our dataset before use.
```

To determine the contributory factors to employee turnover, we can create a number of models and compare any similarities regarding independent variables. 

A second clean of our dataset is in order.
```{r}
#make factors into binary dummy variables and randomize all rows
turnoverMM = as.data.frame(model.matrix(~.-1,turnover)) 

#shuffle rows
set.seed(414)
turnoverMM = turnoverMM[sample(nrow(turnoverMM)),]

#define a normalizing function
normalize = function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

#normalize the data
turnoverNorm = as.data.frame(lapply(turnoverMM, normalize))
```

```{r}
turnoverNorm_no_event <- turnoverNorm
turnoverNorm_no_event$event <- NULL
labels <- turnoverNorm$event
```

The following are a series of baseline models:

## Logistic Regression Analysis
```{r}
library(class)
library(caret)
library(gmodels)

#make the model, remove insignificant elements
logit_model = glm(event ~ . , data = turnoverNorm, family = binomial)
logit_model = step(logit_model, direction = "backward")
summary(logit_model)

#create new vector for prediction
prob_glm = predict(logit_model, newdata = turnoverNorm, type = "response")
pred_glm = ifelse(prob_glm >= 0.5, 1, 0)

#evaluate
CrossTable(x = labels, y = pred_glm, prop.chisq = FALSE)
confusionMatrix(as.factor(labels), as.factor(pred_glm), positive = "1")
```

The GLM model predicted 562 individuals would leave, of which 370 actually left. It missed 192 who were predicted to stay but actually left. The sensitivity is 65.84% and the specificity is 64.55%. For the model overall, the accuracy was 65.19%.

After running our logistic regression model using the step function to determine statistically significant predictors, we ended up keeping the following variables:
- Stag (Experience measured in time)\
- Age\
- Industry\
- Profession\
- Traffic (How the employee got recruited into the company)\
- Coach (How much training the employee received)\
- Way (How employees travel to work: By foot, bus or car)\
- Independent (Amount of task independence)\

Some of the factors that the model eliminated were surprising because we expected gender (insignificant in the final model) and wage to be significant. Logically, mothers and workers earning low wages have more reason to leave the company or workforce in general but that was not reflected in this model.

One factor that the model kept that was surprising was traffic because we didn't expect the method at which the employee joined the company to be significant. This might be because people who were recruited with referrals might be more inclined to stay if they knew people already at the company and have a better support network in the organization.

The confusion matrix shows that the model was only had an accuracy of 65%, with high sensitivity of 66% and specificity of 65%. To answer our business question of which factor affects attrition, this model was not very useful because it has a high quantity of false negatives, misidentifying many employees left as those who were not at risk of leaving the company.

## Decision Tree

```{r}
library(C50)
turnoverNorm$event <- as.factor(turnoverNorm$event)
dt <- C5.0(event ~., data = turnoverNorm)
pred_dt = predict(dt, turnoverNorm)

CrossTable(x = labels, y = pred_dt, prop.chisq = FALSE)
confusionMatrix(as.factor(labels), as.factor(pred_dt), positive = "1")
summary(dt)

```


The decision tree model predicted 568 individuals would leave, of which 516 actually left. It missed 52 who were predicted to stay but actually left. The sensitivity is 90.85% and the specificity is 90.20%. For the model overall, the accuracy was very high, an accuracy of 90.52%, likely a result due to overfitting of our model. 

According to the summary of our decision tree, we found that the industryIT factor seems to be the most important factor in determining employee turnover, followed by industryBuilding, trafficreferal, trafficfriends, and wayfoot. The least important factors were trafficempjs, industryMining, and trafficrabrecNErab. It was pretty interesting for us to see that different types of industries and traffic methods dominate the most and least important factors in predicting turnovers. Meanwhile, individual traits such as age, gender, and personality appear to play as less important roles in affecting turnovers according to our model. 

Combining the results from GLM and decision tree, the two dominating factor that stand out in predicting turnovers are industry and traffic. This conclusion is quite intriguing because it does not align with our intuition of turnovers being determined by more individualistic factors such as personality or happiness. Also, businesses cannot really change what industry they are in, and have limited control in how employees get to employment (traffic). According to these analyses, we raised another question in our mind: Are turnovers less controllable and preventable than we might have assumed? 

## K Nearest Neighbors Analysis
```{r}
library(class)
library(caret)
library(gmodels)

#choose k to be sqrt of the size
kval = sqrt(nrow(turnoverNorm))

prob_knn = knn(train = turnoverNorm_no_event, test = turnoverNorm_no_event, cl = labels, k = kval)

#convert output from binary factor to numeric
pred_knn = ifelse(prob_knn == "1", 1, 0)

#evaluate
CrossTable(x = labels, y = pred_knn, prop.chisq = FALSE)
confusionMatrix(as.factor(labels), as.factor(pred_knn), positive = "1")
```

The KNN model predicted 514 individuals would leave, of which 340 actually left. It missed 174 who were predicted to stay but actually left. The sensitivity is 66.15% and the specificity is 62.44%. Overall, our KNN model reported 64.13% accuracy.

## Artificial Neural Network Analysis
```{r}
library(neuralnet)
library(class)
library(caret)
library(gmodels)

ANN_model = neuralnet(event ~ ., data = turnoverNorm, stepmax = 1e+05) #set low for now for testing
ANN_results = compute(ANN_model, turnoverNorm)
prob_ann = ANN_results$net.result
pred_ann = ifelse(prob_ann >= 0.5, 1, 0)

pred_ann = pred_ann[,2]

#evaluate
CrossTable(x = labels, y = pred_ann, prop.chisq = FALSE)
confusionMatrix(as.factor(labels), as.factor(pred_ann), positive = "1")
```

The ANN model predicted 693 individuals would leave, of which 486 actually left. It missed 207 who were predicted to stay but actually left. The sensitivity is 70.13% and the specificity is 80.50%. Overall, our ANN model was 74.14% accurate.

## SVM Analysis

```{r}
library(kernlab)
svm_model <- ksvm(event ~ ., data = turnoverNorm, kernel = "vanilladot")
pred_SVM <- predict(svm_model, turnoverNorm)

CrossTable(x = labels, y = pred_SVM, prop.chisq = FALSE)
confusionMatrix(as.factor(labels), as.factor(pred_SVM), positive = "1")
```

Our SVM model predicted 529 individuals would leave, of which 362 actually left. It missed 167 who were predicted to stay but actually left. The sensitivity is 68.43% and the specificity is 65.17%. Overall, our SVM model was 66.7% accurate.

## Stacked Model

We now need to create separate testing and training datasets for the stacked model.

```{r}
set.seed(414)
training_indices = sample(1:nrow(turnoverNorm), 0.7*(nrow(turnoverNorm)))

# building a combined dataframe on the five models we produced
combine_df <- data.frame(turnoverNorm$event, pred_glm, pred_knn, pred_ann, pred_dt, pred_SVM)

# splitting into test and train
combine_train = combine_df[training_indices, ]
combine_test = combine_df[-training_indices, ]

```

We now run a decision tree on the combined dataframe.
```{r}
combine_train$turnoverNorm.event <- as.factor(combine_train$turnoverNorm.event)
dt <- C5.0(turnoverNorm.event ~., data = combine_train)
pred_stacked = predict(dt, combine_test)

test_labels <- combine_test$turnoverNorm.event

CrossTable(x = test_labels, y = pred_stacked, prop.chisq = FALSE)
confusionMatrix(as.factor(test_labels), as.factor(pred_stacked), positive = "1")
summary(dt)
```

The stacked model predicted 175 individuals would leave, of which 159 actually left. It missed 16 who were predicted to stay but actually left. The sensitivity is 90.86% and the specificity is 90.85%. Overall, our stacked model reported an accuracy measure of 90.86%. 

According to the summary, the stacked model only used the decision tree model, meaning the decision tree model outperformed all the other models in our analysis. Since we split the data into test and train datasets, the initial concern of overfitting might be mitigated. This leads to an answer for the second part of our business problem, which is that decision tree is most effective in predicting employee turnover given our data. 

#Conclusion

To conclude, we were able to achieve our goals of (1) identifying contributory factors in employee turnover, and (2) determining the most accurate models. The most important factors are industry and traffic, and the most useful model is decision tree (or stacked model which was solely based off decision tree). From our results, IT, building, and banking industries have the highest effect on turnovers. There is a lowest possibility of turnover in IT industry, and highest possibilities in building and banking. For future decision making, industries that are in industries of high turnovers might find it helpful to compare themselves with industries with low turnovers and further analyze the differences. An overall improvement in the industry might be more useful than navigating through individualistic factors in terms of preventing turnovers.
